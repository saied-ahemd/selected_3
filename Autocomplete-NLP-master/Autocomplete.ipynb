{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadingData():\n",
    "    \n",
    "    def __init__(self):\n",
    "        train_file_path = os.path.join(\"Train\")\n",
    "        validation_file_path = os.path.join(\"Validate\")\n",
    "        category_id = 0\n",
    "        self.cat_to_intent = {}\n",
    "        self.intent_to_cat = {}\n",
    "\n",
    "        for dirname, _, filenames in os.walk(train_file_path):\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirname, filename)\n",
    "                intent_id = filename.replace(\".json\", \"\")\n",
    "                self.cat_to_intent[category_id] = intent_id\n",
    "                self.intent_to_cat[intent_id] = category_id\n",
    "                category_id += 1\n",
    "        print(self.cat_to_intent)\n",
    "        print(self.intent_to_cat)\n",
    "        '''Training data'''\n",
    "        training_data = list()\n",
    "        for dirname, _, filenames in os.walk(train_file_path):\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirname, filename)\n",
    "                intent_id = filename.replace(\".json\", \"\")\n",
    "                training_data += self.make_data_for_intent_from_json(\n",
    "                    file_path, intent_id, self.intent_to_cat[intent_id])\n",
    "        self.train_data_frame = pd.DataFrame(\n",
    "            training_data, columns=['Text', 'intent', 'index'])\n",
    "\n",
    "        self.train_data_frame = self.train_data_frame.sample(frac=1)\n",
    "\n",
    "        '''Validation data'''\n",
    "        validation_data = list()\n",
    "        for dirname, _, filenames in os.walk(validation_file_path):\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirname, filename)\n",
    "                intent_id = filename.replace(\".json\", \"\")\n",
    "                validation_data += self.make_data_for_intent_from_json(\n",
    "                    file_path, intent_id, self.intent_to_cat[intent_id])\n",
    "        self.validation_data_frame = pd.DataFrame(\n",
    "            validation_data, columns=['Text', 'intent', 'index'])\n",
    "\n",
    "        self.validation_data_frame = self.validation_data_frame.sample(frac=1)\n",
    "\n",
    "    def make_data_for_intent_from_json(self, json_file, intent_id, cat):\n",
    "        json_d = json.load(open(json_file))\n",
    "\n",
    "        json_dict = json_d[intent_id]\n",
    "\n",
    "        sent_list = list()\n",
    "        for i in json_dict:\n",
    "            each_list = i['data']\n",
    "            sent = \"\"\n",
    "            for i in each_list:\n",
    "                sent = sent + i['text'] + \" \"\n",
    "            sent = sent[:-1]\n",
    "            for i in range(3):\n",
    "                sent = sent.replace(\"  \", \" \")\n",
    "            sent_list.append((sent, intent_id, cat))\n",
    "        return sent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'AddToPlaylist', 1: 'BookRestaurant', 2: 'GetWeather', 3: 'PlayMusic', 4: 'RateBook', 5: 'SearchCreativeWork', 6: 'SearchScreeningEvent'}\n",
      "{'AddToPlaylist': 0, 'BookRestaurant': 1, 'GetWeather': 2, 'PlayMusic': 3, 'RateBook': 4, 'SearchCreativeWork': 5, 'SearchScreeningEvent': 6}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>intent</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>Tell me the weather forecast for my current sp...</td>\n",
       "      <td>GetWeather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>Add another tune to my verano playlist.</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>Is it chilly in Ecola State Park at 2 pm</td>\n",
       "      <td>GetWeather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9372</th>\n",
       "      <td>give three out of 6 points to this textbook</td>\n",
       "      <td>RateBook</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>book a table in Arizona serving italian-americ...</td>\n",
       "      <td>BookRestaurant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text          intent  index\n",
       "4187  Tell me the weather forecast for my current sp...      GetWeather      2\n",
       "1413           Add another tune to my verano playlist.    AddToPlaylist      0\n",
       "4027           Is it chilly in Ecola State Park at 2 pm      GetWeather      2\n",
       "9372        give three out of 6 points to this textbook        RateBook      4\n",
       "2275  book a table in Arizona serving italian-americ...  BookRestaurant      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_obj = LoadingData()\n",
    "df= pd.DataFrame(load_data_obj.train_data_frame)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataFrameList(df, target_column, separator):\n",
    "    \n",
    "    def split_text(line, separator):\n",
    "        splited_line = [e+df for e in line.split(separator) if e]\n",
    "        return splited_line\n",
    "\n",
    "    def splitListToRows(row, row_accumulator, target_column, separator):\n",
    "        split_row = row[target_column].split(separator)\n",
    "        for s in split_row:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows, axis=1, args=(\n",
    "        new_rows, target_column, separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autocompleter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process_data(self, new_df):\n",
    "\n",
    "        print(\"split sentenses on punctuation...\")\n",
    "        for sep in ['. ', ', ', '? ', '! ', '; ']:\n",
    "            new_df = splitDataFrameList(new_df, 'Text', sep)\n",
    "\n",
    "        print(\"Text Cleaning using simple regex...\")\n",
    "        new_df['Text'] = new_df['Text'].apply(lambda x: \" \".join(x.split()))\n",
    "        new_df['Text'] = new_df['Text'].apply(lambda x: x.strip(\".\"))\n",
    "        new_df['Text'] = new_df['Text'].apply(lambda x: \" \".join(x.split()))\n",
    "        new_df['Text'] = new_df['Text'].apply(\n",
    "            lambda x: x.replace(' i ', ' I '))\n",
    "        new_df['Text'] = new_df['Text'].apply(lambda x: x.replace(' ?', '?'))\n",
    "        new_df['Text'] = new_df['Text'].apply(lambda x: x.replace(' !', '!'))\n",
    "        new_df['Text'] = new_df['Text'].apply(lambda x: x.replace(' .', '.'))\n",
    "        new_df['Text'] = new_df['Text'].apply(lambda x: x.replace('OK', 'Ok'))\n",
    "        # new_df['Text'] = new_df['Text'].apply(lambda x: x[0].upper()+x[1:])\n",
    "        new_df['Text'] = new_df['Text'].apply(\n",
    "            lambda x: x+\"?\" if re.search(r'^(Wh|How).+([^?])$', x) else x)\n",
    "\n",
    "        print(\"calculate nb words of sentenses...\")\n",
    "        new_df['nb_words'] = new_df['Text'].apply(\n",
    "            lambda x: len(str(x).split(' ')))\n",
    "        new_df = new_df[new_df['nb_words'] > 2]\n",
    "\n",
    "        print(\"count occurence of sentenses...\")\n",
    "        new_df['Counts'] = new_df.groupby(['Text'])['Text'].transform('count')\n",
    "\n",
    "        print(\"remove duplicates (keep last)...\")\n",
    "        new_df = new_df.drop_duplicates(subset=['Text'], keep='last')\n",
    "\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        print(new_df.shape)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "    def calc_matrice(self, df):\n",
    "        # define tfidf parameter in order to count/vectorize the description vector and then normalize it.\n",
    "        model_tf = TfidfVectorizer(\n",
    "            analyzer='word', ngram_range=(1, 5), min_df=0)\n",
    "        tfidf_matrice = model_tf.fit_transform(df['Text'])\n",
    "        print(\"tfidf_matrice \", tfidf_matrice.shape)\n",
    "        return model_tf, tfidf_matrice\n",
    "\n",
    "    def generate_completions(self, prefix_string, data, model_tf, tfidf_matrice):\n",
    "\n",
    "        prefix_string = str(prefix_string)\n",
    "        new_df = data.reset_index(drop=True)\n",
    "        weights = new_df['Counts'].apply(lambda x: 1 + np.log1p(x)).values\n",
    "\n",
    "        # tranform the string using the tfidf model\n",
    "        tfidf_matrice_spelling = model_tf.transform([prefix_string])\n",
    "        # calculate cosine_matrix\n",
    "        cosine_similarite = linear_kernel(\n",
    "            tfidf_matrice, tfidf_matrice_spelling)\n",
    "\n",
    "        # sort by order of similarity from 1 to 0:\n",
    "        similarity_scores = list(enumerate(cosine_similarite))\n",
    "        similarity_scores = sorted(\n",
    "            similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "        similarity_scores = similarity_scores[0:10]\n",
    "\n",
    "        similarity_scores = [i for i in similarity_scores]\n",
    "        similarity_indices = [i[0] for i in similarity_scores]\n",
    "\n",
    "        # add weight to the potential results that had high frequency in orig data\n",
    "        for i in range(len(similarity_scores)):\n",
    "            similarity_scores[i][1][0] = similarity_scores[i][1][0] * \\\n",
    "                weights[similarity_indices][i]\n",
    "\n",
    "        similarity_scores = sorted(\n",
    "            similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "        similarity_scores = similarity_scores[0:3]\n",
    "        similarity_indices_w = [i[0] for i in similarity_scores]\n",
    "\n",
    "        return new_df.loc[similarity_indices_w]['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sentenses on punctuation...\n",
      "Text Cleaning using simple regex...\n",
      "calculate nb words of sentenses...\n",
      "count occurence of sentenses...\n",
      "remove duplicates (keep last)...\n",
      "(14025, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14025, 5),\n",
       " Index(['Text', 'intent', 'index', 'nb_words', 'Counts'], dtype='object'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocompl = Autocompleter()\n",
    "new_df = autocompl.process_data(df)\n",
    "new_df.shape, new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_matrice  (14025, 202207)\n"
     ]
    }
   ],
   "source": [
    "model_tf, tfidf_matrice = autocompl.calc_matrice(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play Music     \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Play music on Spotify', 'Play music on Itunes', 'play music on Netflix']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'Play Music'\n",
    "\n",
    "print(prefix,\"    \\n \")\n",
    "\n",
    "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Creative Work     \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Find the creative work Brilliant',\n",
       " 'Please search the work',\n",
       " 'I need to find the creative work Wave']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'Search Creative Work'\n",
    "\n",
    "print(prefix,\"    \\n \")\n",
    "\n",
    "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Restaurant     \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Book a restaurant', 'book a restaurant for 6', 'book a restaurant for 8']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'Book Restaurant'\n",
    "\n",
    "print(prefix,\"    \\n \")\n",
    "\n",
    "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import copy\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "021b30a5e9312ccc34815c6364a29effeffb2f9d2108cc8a293ee3fdc577336e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
